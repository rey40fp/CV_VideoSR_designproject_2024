{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import SRCNN, SRCNN_video, SRCNN_video_v2\n",
    "from datasets import TrainDataset, EvalDataset, TrainDataset_3D, EvalDataset_3D,TrainDataset_3D_multifile\n",
    "from utils import AverageMeter, calc_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/20: 100%|██████████| 1494000/1494000 [15:01<00:00, 1658.13it/s, loss=0.110755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval psnr: 21.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2/20: 100%|██████████| 1494000/1494000 [15:17<00:00, 1628.54it/s, loss=0.007828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval psnr: 22.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3/20:   4%|▍         | 63000/1494000 [02:04<26:05, 914.28it/s, loss=0.004627]  "
     ]
    }
   ],
   "source": [
    "# num_workers = 8\n",
    "# all the arguments in variables with their default values\n",
    "# train_file = 'original_training_data/x4/91-image_x4.h5' #TODO\n",
    "# train_file = 'original_training_data/for_training/AMVTG_004.h5' #TODO\n",
    "\n",
    "# train_files  = ['original_training_data/for_training/RMVTG_024.h5', 'original_training_data/for_training/philips_hkc04_002.h5', 'original_training_data/for_training/philips_hkc11_001.h5']\n",
    "train_files  = ['original_training_data/for_training/RMVTG_024.h5',\n",
    "\n",
    "                'original_training_data/for_training/philips_hkc04_002.h5',\n",
    "\n",
    "                'original_training_data/for_training/philips_hkc11_001.h5',\n",
    "\n",
    "                'original_training_data/for_training/HKVTG_004.h5',\n",
    "\n",
    "                'original_training_data/for_training/LDVTG_009.h5',\n",
    "\n",
    "                'original_training_data/for_training/LDVTG_022.h5',\n",
    "\n",
    "                'original_training_data/for_training/NYVTG_006.h5',\n",
    "\n",
    "                'original_training_data/for_training/PRVTG_008.h5',\n",
    "\n",
    "                'original_training_data/for_training/PRVTG_012.h5',\n",
    "\n",
    "                'original_training_data/for_training/RMVTG_011.h5',\n",
    "\n",
    "                'original_training_data/for_training/RMVTG_024.h5',\n",
    "\n",
    "                'original_training_data/for_training/TPVTG_003.h5',\n",
    "\n",
    "                'original_training_data/for_training/cact1_001.h5',\n",
    "\n",
    "                'original_training_data/for_training/car05_001.h5',\n",
    "\n",
    "                'original_training_data/for_training/gree3_001.h5',\n",
    "\n",
    "                'original_training_data/for_training/hdclub_001_002.h5',\n",
    "\n",
    "                'original_training_data/for_training/hdclub_008_007.h5',\n",
    "\n",
    "                'original_training_data/for_training/hitachi_isee5_001.h5',\n",
    "\n",
    "                'original_training_data/for_training/hk001_001.h5',\n",
    "\n",
    "                'original_training_data/for_training/philips_hkc01_001.h5',\n",
    "                ]\n",
    "train_file = 'original_training_data/for_training/philips_hkc04_002.h5' #TODO\n",
    "# eval_file = 'original_training_data/x4/Set5_x4.h5' #TODO\n",
    "eval_file = 'preparing_data/prepare_out_3d/for_eval/AMVTG_004.h5' #TODO\n",
    "# eval_file = 'original_training_data/for_training/car05_001.h5' #TODO\n",
    "outputs_dir = 'outputs'\n",
    "scale = 4\n",
    "lr = 3e-4\n",
    "batch_size = 9000\n",
    "num_epochs = 20\n",
    "num_workers = 12\n",
    "seed = 123\n",
    "\n",
    "# new output dir using the statics variables\n",
    "outputs_dir = os.path.join(outputs_dir, 'x{}'.format(scale))\n",
    "\n",
    "# if not os.path.exists(args.outputs_dir):\n",
    "#     os.makedirs(args.outputs_dir)\n",
    "\n",
    "\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = SRCNN_video_v2().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.conv1.parameters()},\n",
    "    {'params': model.conv2.parameters()},\n",
    "    {'params': model.conv3.parameters(), 'lr': lr * 0.1}\n",
    "], lr=lr)\n",
    "\n",
    "# train_dataset = TrainDataset_3D(train_file)\n",
    "# train_dataset = TrainDataset_3D_multifile(train_files) # CHANGE HERE\n",
    "\n",
    "train_dataset_1 = TrainDataset_3D(train_files[0]) # CHANGE HERE\n",
    "# train_dataset_2 = TrainDataset_3D(train_files[1]) # CHANGE HERE\n",
    "# train_dataset_3 = TrainDataset_3D(train_files[2]) # CHANGE HERE\n",
    "\n",
    "# Lets create a foor look to concatenate all the train datasets\n",
    "\n",
    "for i in range(1, len(train_files)):\n",
    "    train_dataset = TrainDataset_3D(train_files[i])\n",
    "    train_dataset_1 = ConcatDataset([train_dataset_1, train_dataset])\n",
    "\n",
    "\n",
    "# concatenated = ConcatDataset([train_dataset_1, train_dataset_2])\n",
    "\n",
    "# for loop to print number of elements in each train dataset\n",
    "# for i in range(len(train_files)):\n",
    "#     train_dataset = TrainDataset_3D(train_files[i])\n",
    "#     print('Number of elements in train_dataset_{}: {}'.format(i, len(train_dataset)))\n",
    "\n",
    "\n",
    "\n",
    "# concatenated = ConcatDataset([train_dataset_1, train_dataset_2, train_dataset_3])\n",
    "    \n",
    "concatenated = train_dataset_1\n",
    "train_dataloader = DataLoader(dataset=concatenated,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=True,\n",
    "                                drop_last=True)\n",
    "eval_dataset = EvalDataset_3D(eval_file)\n",
    "eval_dataloader = DataLoader(dataset=eval_dataset, batch_size=1)\n",
    "\n",
    "best_weights = copy.deepcopy(model.state_dict())\n",
    "best_epoch = 0\n",
    "best_psnr = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = AverageMeter()\n",
    "    \n",
    "    # Before\n",
    "    # total=(len(train_dataset) - len(train_dataset) % batch_size)\n",
    "    # New total\n",
    "    # total = len(train_dataset_1) + len(train_dataset_2) - (len(train_dataset_1) + len(train_dataset_2)) % batch_size\n",
    "\n",
    "    #Train_files has way too mny files so lets do a one line loop to get the total \n",
    "    # total = len(train_dataset_1) + len(train_dataset_2) + len(train_dataset_3) - (len(train_dataset_1) + len(train_dataset_2) + len(train_dataset_3)) % batch_size\n",
    "\n",
    "    with tqdm(total = (len(train_dataset_1) - (len(train_dataset_1)) % batch_size)) as t:\n",
    "        t.set_description('epoch: {}/{}'.format(epoch+1, num_epochs))\n",
    "\n",
    "        for data in train_dataloader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            epoch_losses.update(loss.item(), len(inputs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n",
    "            t.update(len(inputs))\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(outputs_dir, 'epoch_{}.pth'.format(epoch)))\n",
    "\n",
    "    model.eval()\n",
    "    epoch_psnr = AverageMeter()\n",
    "\n",
    "    for data in eval_dataloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs).clamp(0.0, 1.0)\n",
    "\n",
    "        epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n",
    "\n",
    "    print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n",
    "\n",
    "    if epoch_psnr.avg > best_psnr:\n",
    "        best_epoch = epoch\n",
    "        best_psnr = epoch_psnr.avg\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\n",
    "torch.save(best_weights, os.path.join(outputs_dir, 'best.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL DATA\n",
      "All keys: <KeysViewHDF5 ['hr', 'lr', 'prev_lr']> \n",
      "hr: Group\n",
      "All keys: <KeysViewHDF5 ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '4', '5', '6', '7', '8', '9']> \n",
      "    1: Dataset with shape (540, 960) and data type float32\n",
      "    10: Dataset with shape (540, 960) and data type float32\n",
      "    11: Dataset with shape (540, 960) and data type float32\n",
      "    12: Dataset with shape (540, 960) and data type float32\n",
      "    13: Dataset with shape (540, 960) and data type float32\n",
      "    14: Dataset with shape (540, 960) and data type float32\n",
      "    15: Dataset with shape (540, 960) and data type float32\n",
      "    16: Dataset with shape (540, 960) and data type float32\n",
      "    17: Dataset with shape (540, 960) and data type float32\n",
      "    18: Dataset with shape (540, 960) and data type float32\n",
      "    19: Dataset with shape (540, 960) and data type float32\n",
      "    2: Dataset with shape (540, 960) and data type float32\n",
      "    20: Dataset with shape (540, 960) and data type float32\n",
      "    21: Dataset with shape (540, 960) and data type float32\n",
      "    22: Dataset with shape (540, 960) and data type float32\n",
      "    23: Dataset with shape (540, 960) and data type float32\n",
      "    24: Dataset with shape (540, 960) and data type float32\n",
      "    25: Dataset with shape (540, 960) and data type float32\n",
      "    26: Dataset with shape (540, 960) and data type float32\n",
      "    27: Dataset with shape (540, 960) and data type float32\n",
      "    28: Dataset with shape (540, 960) and data type float32\n",
      "    29: Dataset with shape (540, 960) and data type float32\n",
      "    3: Dataset with shape (540, 960) and data type float32\n",
      "    30: Dataset with shape (540, 960) and data type float32\n",
      "    4: Dataset with shape (540, 960) and data type float32\n",
      "    5: Dataset with shape (540, 960) and data type float32\n",
      "    6: Dataset with shape (540, 960) and data type float32\n",
      "    7: Dataset with shape (540, 960) and data type float32\n",
      "    8: Dataset with shape (540, 960) and data type float32\n",
      "    9: Dataset with shape (540, 960) and data type float32\n",
      "lr: Group\n",
      "All keys: <KeysViewHDF5 ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '4', '5', '6', '7', '8', '9']> \n",
      "    1: Dataset with shape (540, 960) and data type float32\n",
      "    10: Dataset with shape (540, 960) and data type float32\n",
      "    11: Dataset with shape (540, 960) and data type float32\n",
      "    12: Dataset with shape (540, 960) and data type float32\n",
      "    13: Dataset with shape (540, 960) and data type float32\n",
      "    14: Dataset with shape (540, 960) and data type float32\n",
      "    15: Dataset with shape (540, 960) and data type float32\n",
      "    16: Dataset with shape (540, 960) and data type float32\n",
      "    17: Dataset with shape (540, 960) and data type float32\n",
      "    18: Dataset with shape (540, 960) and data type float32\n",
      "    19: Dataset with shape (540, 960) and data type float32\n",
      "    2: Dataset with shape (540, 960) and data type float32\n",
      "    20: Dataset with shape (540, 960) and data type float32\n",
      "    21: Dataset with shape (540, 960) and data type float32\n",
      "    22: Dataset with shape (540, 960) and data type float32\n",
      "    23: Dataset with shape (540, 960) and data type float32\n",
      "    24: Dataset with shape (540, 960) and data type float32\n",
      "    25: Dataset with shape (540, 960) and data type float32\n",
      "    26: Dataset with shape (540, 960) and data type float32\n",
      "    27: Dataset with shape (540, 960) and data type float32\n",
      "    28: Dataset with shape (540, 960) and data type float32\n",
      "    29: Dataset with shape (540, 960) and data type float32\n",
      "    3: Dataset with shape (540, 960) and data type float32\n",
      "    30: Dataset with shape (540, 960) and data type float32\n",
      "    4: Dataset with shape (540, 960) and data type float32\n",
      "    5: Dataset with shape (540, 960) and data type float32\n",
      "    6: Dataset with shape (540, 960) and data type float32\n",
      "    7: Dataset with shape (540, 960) and data type float32\n",
      "    8: Dataset with shape (540, 960) and data type float32\n",
      "    9: Dataset with shape (540, 960) and data type float32\n",
      "prev_lr: Group\n",
      "All keys: <KeysViewHDF5 ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '4', '5', '6', '7', '8', '9']> \n",
      "    1: Dataset with shape (540, 960) and data type float32\n",
      "    10: Dataset with shape (540, 960) and data type float32\n",
      "    11: Dataset with shape (540, 960) and data type float32\n",
      "    12: Dataset with shape (540, 960) and data type float32\n",
      "    13: Dataset with shape (540, 960) and data type float32\n",
      "    14: Dataset with shape (540, 960) and data type float32\n",
      "    15: Dataset with shape (540, 960) and data type float32\n",
      "    16: Dataset with shape (540, 960) and data type float32\n",
      "    17: Dataset with shape (540, 960) and data type float32\n",
      "    18: Dataset with shape (540, 960) and data type float32\n",
      "    19: Dataset with shape (540, 960) and data type float32\n",
      "    2: Dataset with shape (540, 960) and data type float32\n",
      "    20: Dataset with shape (540, 960) and data type float32\n",
      "    21: Dataset with shape (540, 960) and data type float32\n",
      "    22: Dataset with shape (540, 960) and data type float32\n",
      "    23: Dataset with shape (540, 960) and data type float32\n",
      "    24: Dataset with shape (540, 960) and data type float32\n",
      "    25: Dataset with shape (540, 960) and data type float32\n",
      "    26: Dataset with shape (540, 960) and data type float32\n",
      "    27: Dataset with shape (540, 960) and data type float32\n",
      "    28: Dataset with shape (540, 960) and data type float32\n",
      "    29: Dataset with shape (540, 960) and data type float32\n",
      "    3: Dataset with shape (540, 960) and data type float32\n",
      "    30: Dataset with shape (540, 960) and data type float32\n",
      "    4: Dataset with shape (540, 960) and data type float32\n",
      "    5: Dataset with shape (540, 960) and data type float32\n",
      "    6: Dataset with shape (540, 960) and data type float32\n",
      "    7: Dataset with shape (540, 960) and data type float32\n",
      "    8: Dataset with shape (540, 960) and data type float32\n",
      "    9: Dataset with shape (540, 960) and data type float32\n",
      "TRAINING DATA\n",
      "All keys: <KeysViewHDF5 ['hr', 'lr', 'prev_lr']> \n",
      "hr: Dataset with shape (74370, 33, 33) and data type float32\n",
      "lr: Dataset with shape (74370, 33, 33) and data type float32\n",
      "prev_lr: Dataset with shape (74370, 33, 33) and data type float32\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def print_structure(h5_file, indent=''):\n",
    "    \"\"\"\n",
    "    Recursively prints the structure of an HDF5 file along with dataset shapes and data types.\n",
    "    \"\"\"\n",
    "    print(f\"All keys: {h5_file.keys()} \")\n",
    "    for key in h5_file.keys():   \n",
    "        item = h5_file[key]\n",
    "        print(f'{indent}{key}: ', end='')\n",
    "        if isinstance(item, h5py.Dataset):  # Check if the item is a dataset\n",
    "            print(f'Dataset with shape {item.shape} and data type {item.dtype}')\n",
    "        elif isinstance(item, h5py.Group):  # Check if the item is a group\n",
    "            print(f'Group')\n",
    "            print_structure(item, indent + '    ')  # Recurse into the group with increased indentation\n",
    "\n",
    "# Usage example\n",
    "            \n",
    "            #EVAL DATA\n",
    "# print('EVAL DATA')\n",
    "# with h5py.File('original_training_data/x4/Set5_x4.h5', 'r') as file:\n",
    "#     print_structure(file)\n",
    "#             # TRAINING DATA\n",
    "# print('TRAINING DATA')\n",
    "# with h5py.File('original_training_data/x4/91-image_x4.h5', 'r') as file:\n",
    "#     print_structure(file)\n",
    "\n",
    "\n",
    "\n",
    "print('EVAL DATA')\n",
    "with h5py.File('preparing_data/prepare_out_3d/for_eval/AMVTG_004.h5', 'r') as file:\n",
    "    print_structure(file)\n",
    "            # TRAINING DATA\n",
    "print('TRAINING DATA')\n",
    "with h5py.File('original_training_data/for_training/AMVTG_004.h5', 'r') as file:\n",
    "    print_structure(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
