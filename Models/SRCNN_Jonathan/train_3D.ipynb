{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TrainDataset' from 'datasets' (/Users/j/Documents/GitHub/CV_VideoSR_designproject_2024/Models/SRCNN_Jonathan/datasets.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SRCNN, SRCNN_video\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainDataset, EvalDataset, TrainDataset_3D, EvalDataset_3D\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AverageMeter, calc_psnr\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TrainDataset' from 'datasets' (/Users/j/Documents/GitHub/CV_VideoSR_designproject_2024/Models/SRCNN_Jonathan/datasets.py)"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import SRCNN, SRCNN_video\n",
    "from datasets import TrainDataset, EvalDataset, TrainDataset_3D, EvalDataset_3D\n",
    "from utils import AverageMeter, calc_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from h5py) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m\n\u001b[1;32m     33\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     34\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam([\n\u001b[1;32m     35\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mparameters()},\n\u001b[1;32m     36\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mparameters()},\n\u001b[1;32m     37\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mconv3\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m}\n\u001b[1;32m     38\u001b[0m ], lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m---> 40\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTrainDataset\u001b[49m(train_file) \u001b[38;5;66;03m# CHANGE HERE\u001b[39;00m\n\u001b[1;32m     41\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     42\u001b[0m                                 batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     43\u001b[0m                                 shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m                                 num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m     45\u001b[0m                                 pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m                                 drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m EvalDataset(eval_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# num_workers = 8\n",
    "# all the arguments in variables with their default values\n",
    "train_file = 'original_training_data/x4/91-image_x4.h5' #TODO\n",
    "#train_file = 'original_training_data/for_training/AMVTG_004.h5' #TODO\n",
    "eval_file = 'original_training_data/x4/Set5_x4.h5' #TODO\n",
    "#eval_file = 'preparing_data/prepare_out_3d/for_eval/AMVTG_004.h5' #TODO\n",
    "# eval_file = 'original_training_data/for_training/car05_001.h5' #TODO\n",
    "outputs_dir = 'outputs'\n",
    "scale = 4\n",
    "lr = 1e-4\n",
    "batch_size = 320\n",
    "num_epochs = 3\n",
    "num_workers = 12\n",
    "seed = 123\n",
    "\n",
    "# new output dir using the statics variables\n",
    "outputs_dir = os.path.join(outputs_dir, 'x{}'.format(scale))\n",
    "\n",
    "# if not os.path.exists(args.outputs_dir):\n",
    "#     os.makedirs(args.outputs_dir)\n",
    "\n",
    "\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = SRCNN_video().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.conv1.parameters()},\n",
    "    {'params': model.conv2.parameters()},\n",
    "    {'params': model.conv3.parameters(), 'lr': lr * 0.1}\n",
    "], lr=lr)\n",
    "\n",
    "train_dataset = TrainDataset(train_file) # CHANGE HERE\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=True,\n",
    "                                drop_last=True)\n",
    "eval_dataset = EvalDataset(eval_file)\n",
    "eval_dataloader = DataLoader(dataset=eval_dataset, batch_size=1)\n",
    "\n",
    "best_weights = copy.deepcopy(model.state_dict())\n",
    "best_epoch = 0\n",
    "best_psnr = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(len(train_dataset) - len(train_dataset) % batch_size)) as t:\n",
    "        t.set_description('epoch: {}/{}'.format(epoch+1, num_epochs))\n",
    "\n",
    "        for data in train_dataloader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            epoch_losses.update(loss.item(), len(inputs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n",
    "            t.update(len(inputs))\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(outputs_dir, 'epoch_{}.pth'.format(epoch)))\n",
    "\n",
    "    model.eval()\n",
    "    epoch_psnr = AverageMeter()\n",
    "\n",
    "    for data in eval_dataloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs).clamp(0.0, 1.0)\n",
    "\n",
    "        epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n",
    "\n",
    "    print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n",
    "\n",
    "    if epoch_psnr.avg > best_psnr:\n",
    "        best_epoch = epoch\n",
    "        best_psnr = epoch_psnr.avg\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\n",
    "torch.save(best_weights, os.path.join(outputs_dir, 'best.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL DATA\n",
      "All keys: <KeysViewHDF5 ['hr', 'lr', 'prev_lr']> \n",
      "hr: Group\n",
      "All keys: <KeysViewHDF5 ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '4', '5', '6', '7', '8', '9']> \n",
      "    1: Dataset with shape (540, 960) and data type float32\n",
      "    10: Dataset with shape (540, 960) and data type float32\n",
      "    11: Dataset with shape (540, 960) and data type float32\n",
      "    12: Dataset with shape (540, 960) and data type float32\n",
      "    13: Dataset with shape (540, 960) and data type float32\n",
      "    14: Dataset with shape (540, 960) and data type float32\n",
      "    15: Dataset with shape (540, 960) and data type float32\n",
      "    16: Dataset with shape (540, 960) and data type float32\n",
      "    17: Dataset with shape (540, 960) and data type float32\n",
      "    18: Dataset with shape (540, 960) and data type float32\n",
      "    19: Dataset with shape (540, 960) and data type float32\n",
      "    2: Dataset with shape (540, 960) and data type float32\n",
      "    20: Dataset with shape (540, 960) and data type float32\n",
      "    21: Dataset with shape (540, 960) and data type float32\n",
      "    22: Dataset with shape (540, 960) and data type float32\n",
      "    23: Dataset with shape (540, 960) and data type float32\n",
      "    24: Dataset with shape (540, 960) and data type float32\n",
      "    25: Dataset with shape (540, 960) and data type float32\n",
      "    26: Dataset with shape (540, 960) and data type float32\n",
      "    27: Dataset with shape (540, 960) and data type float32\n",
      "    28: Dataset with shape (540, 960) and data type float32\n",
      "    29: Dataset with shape (540, 960) and data type float32\n",
      "    3: Dataset with shape (540, 960) and data type float32\n",
      "    30: Dataset with shape (540, 960) and data type float32\n",
      "    4: Dataset with shape (540, 960) and data type float32\n",
      "    5: Dataset with shape (540, 960) and data type float32\n",
      "    6: Dataset with shape (540, 960) and data type float32\n",
      "    7: Dataset with shape (540, 960) and data type float32\n",
      "    8: Dataset with shape (540, 960) and data type float32\n",
      "    9: Dataset with shape (540, 960) and data type float32\n",
      "lr: Group\n",
      "All keys: <KeysViewHDF5 ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '4', '5', '6', '7', '8', '9']> \n",
      "    1: Dataset with shape (540, 960) and data type float32\n",
      "    10: Dataset with shape (540, 960) and data type float32\n",
      "    11: Dataset with shape (540, 960) and data type float32\n",
      "    12: Dataset with shape (540, 960) and data type float32\n",
      "    13: Dataset with shape (540, 960) and data type float32\n",
      "    14: Dataset with shape (540, 960) and data type float32\n",
      "    15: Dataset with shape (540, 960) and data type float32\n",
      "    16: Dataset with shape (540, 960) and data type float32\n",
      "    17: Dataset with shape (540, 960) and data type float32\n",
      "    18: Dataset with shape (540, 960) and data type float32\n",
      "    19: Dataset with shape (540, 960) and data type float32\n",
      "    2: Dataset with shape (540, 960) and data type float32\n",
      "    20: Dataset with shape (540, 960) and data type float32\n",
      "    21: Dataset with shape (540, 960) and data type float32\n",
      "    22: Dataset with shape (540, 960) and data type float32\n",
      "    23: Dataset with shape (540, 960) and data type float32\n",
      "    24: Dataset with shape (540, 960) and data type float32\n",
      "    25: Dataset with shape (540, 960) and data type float32\n",
      "    26: Dataset with shape (540, 960) and data type float32\n",
      "    27: Dataset with shape (540, 960) and data type float32\n",
      "    28: Dataset with shape (540, 960) and data type float32\n",
      "    29: Dataset with shape (540, 960) and data type float32\n",
      "    3: Dataset with shape (540, 960) and data type float32\n",
      "    30: Dataset with shape (540, 960) and data type float32\n",
      "    4: Dataset with shape (540, 960) and data type float32\n",
      "    5: Dataset with shape (540, 960) and data type float32\n",
      "    6: Dataset with shape (540, 960) and data type float32\n",
      "    7: Dataset with shape (540, 960) and data type float32\n",
      "    8: Dataset with shape (540, 960) and data type float32\n",
      "    9: Dataset with shape (540, 960) and data type float32\n",
      "prev_lr: Group\n",
      "All keys: <KeysViewHDF5 ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '4', '5', '6', '7', '8', '9']> \n",
      "    1: Dataset with shape (540, 960) and data type float32\n",
      "    10: Dataset with shape (540, 960) and data type float32\n",
      "    11: Dataset with shape (540, 960) and data type float32\n",
      "    12: Dataset with shape (540, 960) and data type float32\n",
      "    13: Dataset with shape (540, 960) and data type float32\n",
      "    14: Dataset with shape (540, 960) and data type float32\n",
      "    15: Dataset with shape (540, 960) and data type float32\n",
      "    16: Dataset with shape (540, 960) and data type float32\n",
      "    17: Dataset with shape (540, 960) and data type float32\n",
      "    18: Dataset with shape (540, 960) and data type float32\n",
      "    19: Dataset with shape (540, 960) and data type float32\n",
      "    2: Dataset with shape (540, 960) and data type float32\n",
      "    20: Dataset with shape (540, 960) and data type float32\n",
      "    21: Dataset with shape (540, 960) and data type float32\n",
      "    22: Dataset with shape (540, 960) and data type float32\n",
      "    23: Dataset with shape (540, 960) and data type float32\n",
      "    24: Dataset with shape (540, 960) and data type float32\n",
      "    25: Dataset with shape (540, 960) and data type float32\n",
      "    26: Dataset with shape (540, 960) and data type float32\n",
      "    27: Dataset with shape (540, 960) and data type float32\n",
      "    28: Dataset with shape (540, 960) and data type float32\n",
      "    29: Dataset with shape (540, 960) and data type float32\n",
      "    3: Dataset with shape (540, 960) and data type float32\n",
      "    30: Dataset with shape (540, 960) and data type float32\n",
      "    4: Dataset with shape (540, 960) and data type float32\n",
      "    5: Dataset with shape (540, 960) and data type float32\n",
      "    6: Dataset with shape (540, 960) and data type float32\n",
      "    7: Dataset with shape (540, 960) and data type float32\n",
      "    8: Dataset with shape (540, 960) and data type float32\n",
      "    9: Dataset with shape (540, 960) and data type float32\n",
      "TRAINING DATA\n",
      "All keys: <KeysViewHDF5 ['hr', 'lr', 'prev_lr']> \n",
      "hr: Dataset with shape (74370, 33, 33) and data type float32\n",
      "lr: Dataset with shape (74370, 33, 33) and data type float32\n",
      "prev_lr: Dataset with shape (74370, 33, 33) and data type float32\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def print_structure(h5_file, indent=''):\n",
    "    \"\"\"\n",
    "    Recursively prints the structure of an HDF5 file along with dataset shapes and data types.\n",
    "    \"\"\"\n",
    "    print(f\"All keys: {h5_file.keys()} \")\n",
    "    for key in h5_file.keys():   \n",
    "        item = h5_file[key]\n",
    "        print(f'{indent}{key}: ', end='')\n",
    "        if isinstance(item, h5py.Dataset):  # Check if the item is a dataset\n",
    "            print(f'Dataset with shape {item.shape} and data type {item.dtype}')\n",
    "        elif isinstance(item, h5py.Group):  # Check if the item is a group\n",
    "            print(f'Group')\n",
    "            print_structure(item, indent + '    ')  # Recurse into the group with increased indentation\n",
    "\n",
    "# Usage example\n",
    "            \n",
    "            #EVAL DATA\n",
    "# print('EVAL DATA')\n",
    "# with h5py.File('original_training_data/x4/Set5_x4.h5', 'r') as file:\n",
    "#     print_structure(file)\n",
    "#             # TRAINING DATA\n",
    "# print('TRAINING DATA')\n",
    "# with h5py.File('original_training_data/x4/91-image_x4.h5', 'r') as file:\n",
    "#     print_structure(file)\n",
    "\n",
    "\n",
    "\n",
    "print('EVAL DATA')\n",
    "with h5py.File('preparing_data/prepare_out_3d/for_eval/AMVTG_004.h5', 'r') as file:\n",
    "    print_structure(file)\n",
    "            # TRAINING DATA\n",
    "print('TRAINING DATA')\n",
    "with h5py.File('original_training_data/for_training/AMVTG_004.h5', 'r') as file:\n",
    "    print_structure(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
